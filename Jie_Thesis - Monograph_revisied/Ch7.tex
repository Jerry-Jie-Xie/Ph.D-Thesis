% Ch5.tex

\chapter{Frog call classification based on multi-label learning}
\label{cha:cha7ML}
\textbf{Research problem}
\\
In Chapter \ref{cha:cha6MIML}, the classification performance is highly affected by the syllable segmentation results, which is realised by AED. 
\\
\textbf{Research sub-question}
\\
How to reduce the effect of AED results and classify multiple simultaneously vocalising frog species in low SNR recordings?


\section{Overview}
\label{sect:introduction}

This chapter describes the research conducted for classifying multiple simultaneously vocalising frog species. In Chapter \ref{cha:cha6MIML}, acoustic features are calculated based on AED results, and the MIML classification performance is highly affected by the accuracy of AED results. To reduce the bias introduced by AED, this chapter uses global feature sets for classifying multiple frog species in low-SNR recordings.
To be specific, three features are calculated: LPCs, MFCCs, and PWSCCs. Here, each feature is extracted from the whole 10-second recording without  syllable segmentation. Therefore, the classification process can be naturally framed as a ML learning framework.  



  

\section{Acquisition of frog call recordings}

To evaluate the proposed algorithm, the same dataset with Chapter~\ref{cha:cha6MIML} is used. The description of this dataset can be found in Chapter~\ref{chap5:Materials}. We first manually inspect spectrograms of ten randomly selected call examples for each frog species. Two parameters, dominant frequency and syllable duration, are measured and averaged, as listed in Table~\ref{tab:Ch7_parameters}, which are used as prior information for subsequent analysis.

\section{Feature extraction}


Extracting discriminating features, which maximise between-group (inter-specie) dissimilarity
and minimise within-group (intra-specie) dissimilarity, is very important for achieving high classification performance \citep{huang2009frog, bedoya2014automatic}. In this chapter, three global features are calculated to classify multiple simultaneously vocalising frog species in each 10-second recording: LPCs, MFCCs, and PWSCCs. 





\subsection{LPCs}
Linear prediction coding (LPCs) is often used to represent the spectral envelope of speech sounds \citep{itakura1975line}. LPCs coefficients can be calculated using a linear predictive filter.

\begin{equation}
X(n) = \sum_{i}^{p}a_{i}x(n-i)
\end{equation}
where $p$ is the order of the polynomial $a_{i}$. In the proposed study, the value of $p$ is set at 12 (12th-order polynomial), and 13 LPCs coefficients are calculated. For those frog vocalizations with different spectral envelopes, LPCs can obtain a high classification accuracy, and has been widely used in previous studies \cite{yuan2012frog, jaafarcomparative, jaafar2015effect}.


\subsection{MFCCs}
The description for calculating MFCCs can be found in Chapter~\ref{cha:cha4EnhancedFeature}. The difference is that each 10-second recording is first split into frames using a Hamming window. Then, all frames are divided into three equal parts, and MFCCs is averaged over each part. Finally, MFCCs of each part are concatenated into a final feature named as \textit{multi-stage MFCCs}. 
The reason for dividing the recording into three equal parts is to keep the information of different frog species in the same frequency band, 
because different frog species may exist in different temporal locations of one 10-second recording. 


\subsection{PWSCCs}
To calculate PWSCCs, constructing a suitable frequency scale for a WP tree based on the dominant frequency of each frog species is the first step, because different frog species tend to have different dominant frequencies \citep{Gingras2013}. In Chapter \ref{cha:cha5WaveletFeature}, k-means clustering was first applied to the extracted dominant frequencies of training data. Then, the frequency scale was built by sorting clustering centroids to construct the WP tree. In this chapter, the prior information for dominant frequency ($F_{0}$) (Table~\ref{tab:Ch7_parameters}) is directly used to construct the WP tree. Then, the steps for calculating PWSCCs are the same with Chapter~\ref{cha:cha5WaveletFeature}. Similar with MFCCs, PWSCCs are also calculated by dividing the recording into three equal parts. The final feature is named as \textit{multi-stage PWSCCs}.



\section{Experiment setup}
Each 10-second recording is divided into frames of 512 samples and 50\% frame overlap for STFT. For MFCCs and PWSCCs, window size and overlap are 512 samples and 50\%, the window function is a Hamming window. All algorithms were programmed in Matlab 2014b except ML learning, which was implemented in Meka 1.7.7\footnote[4]{http://meka.sourceforge.net/}. 



\subsection{Multi-label classification} 
Since many sampled recordings consist of calls from multiple frog species, frog call classification can be framed as a ML learning problem. However, previous studies have not adopted ML learning to classify frog calls. Therefore, it is worth investigating different ML learning algorithms for the classification of multiple vocalising frog species in low SNR recordings. for its scalability and flexibility \citep{read2011classifier}. The principle of the BR method is to solve a multi-label classification problem using multiple binary classifiers respectively. Similar to our previous work \citep{zhang2016using}, 
three classic single-label learning algorithms: decision tree, and multi-layer perceptron, and random forest. Here, random forest is selected as the base classifier, since our previous study of classifying frog species has already demonstrated its comparable performance \citep{xie2016acoustic}. 




\subsection{Evaluation metrics}
For multi-label classification, the performance evaluation differs from the classic single-label classification systems. The multi-label classification results often have a situation where partial labels are correctly predicted, but the prediction of single-label classification is either correct or incorrect. Therefore, some traditional evaluation metrics for the single-label classification, such as precision, recall, and accuracy, are no longer suitable for the multi-label classification system. In this study, three evaluation metrics, hamming loss, accuracy, and subset accuracy, are used, where all the three are example based measures \citep{madjarov2012extensive}.


The definition of Hamming loss can be found in Chapter~\ref{cha:cha6MIML}.
%Hamming loss is defined as the fraction of labels that are incorrectly predicted for an instance and the normalised hamming loss which is normalised over instances is reported. This metric is defined as
%\begin{equation}
%hammingLoss = \frac{1}{N}\sum_{i=1}^{N}\frac{1}{Q}|h(x_{i})\Delta y_{i})|
%\end{equation}
%where $\Delta$ denotes the symmetric difference between two instances,  and $Q$ is the total number of possible labels. 

Accuracy for a single instance $x_{i}$ is defined by the Jaccard similarity coefficients between the ground truth $y_{i}$ and the prediction $h(x_{i})$. Accuracy is micro-averaged across all examples:

\begin{equation}
accuracy = \frac{1}{N}\sum_{i=1}^{N}|\frac{h(x_{i} \cap y_{i})}{h(x_{i} \cup y_{i}}|
\end{equation}
where $N$ is the number of instances, $y_{i}$ denotes the ground truth of instance $x_{i}$, and $h(x_{i})$ denotes the predictions for the same instance. 


Subset accuracy is defined as follows:
\begin{equation}
subsetAccuracy = \frac{1}{N}\sum_{i=1}^{N}I(h(x_{i})=y_{i})
\end{equation}
where $I(true)=1$ and $I(false)=0$. This is a very strict evaluation
measure as it requires the predicted set of labels to be an exact
match of the true set of labels.


The values for hamming loss, accuracy, and subset accuracy range from 0 to 1. For hamming loss, 0 denotes the perfect result, and 1 means the wrong prediction of all labels over every instance, whereas for accuracy and subset accuracy, the values have the completely opposite meanings.



\subsection{Classification results}

Experiment results are shown in Table~\ref{tab:classificationResults}.
The combination of multi-stage PWSCCs $+$ LPCs and the RAKEL1 method achieves the best performance. Therefore, this combination is used for the testing data. 
Figure~\ref{fig:frogRichness} shows the frog species richness of the three selected sites. For all the three sites, the variation of species richness is not high, which shows that species richness of the same area is relatively stable. However, frog species richness of \textit{BG Creek dam} has a smaller variation over the time than \textit{Kiyomi dam} and \textit{Stony Creek dam}. The comparison of the species richness for the three sites is shown in Figure~\ref{fig:richnessSite}. In contrast to other sites, the species richness in \textit{BG Creek dam} is the highest. This might be that \textit{BG Creek dam} is closer to a river and farther away from the human community.


\begin{table}[htb!]
\centering
\caption[Classification results based on four feature sets and four ML learning algorithms.]{Classification results based on four feature sets and four ML learning algorithms. Here the methods for ML learning algorithms are in accordance to the name in the \textit{Meka} software. The base classifier of all methods is decision tree. For a metric, the best value is in bold. Here, $\downarrow$ indicates that smaller values imply higher accuracy, while ‘$\uparrow$’ has the completely opposite meanings. The description of each evaluation metric can be found in Chapter \ref{ch6:eveluationMetric}.}
\label{tab:classificationResults}
\resizebox{\textwidth}{!}{
\begin{tabular}{llllllll}
\hhline{========}
\textbf{Features}                  & \textbf{Method} & \textbf{Hamming loss} $\downarrow$ & \textbf{Rank loss} $\downarrow$ & \textbf{One error} $\downarrow$& \textbf{Example based F1} $\uparrow$ & \textbf{Micro F1} $\uparrow$ \\ \hline
MFCCs+LPCs                & BR              & 0.155 $\pm$ 0.015           & 0.171 $\pm$ 0.037        &  0.246 $\pm$ 0.063        & 0.699 $\pm$ 0.03                 & 0.749 $\pm$ 0.024       \\ 
                          & CC              & 0.147 $\pm$ 0.018           & 0.147 $\pm$ 0.02          & 0.199 $\pm$ 0.042        & 0.722 $\pm$ 0.035                & 0.756 $\pm$ 0.029       \\ 
                          & RAKEL           & 0.167 $\pm$ 0.038           & 0.122 $\pm$ 0.026                    & 0.194 $\pm$ 0.063        & 0.721 $\pm$ 0.044                & 0.752 $\pm$ 0.041       \\ 
                          & RAKEL1           & 0.134 $\pm$ 0.012           & 0.099 $\pm$ 0.025       & \textbf{0.147 $\pm$ 0.056}        & 0.74 $\pm$ 0.044                 & 0.783 $\pm$ 0.022       \\ 
Multi-stage MFCCs + LPCs  & BR              & 0.155 $\pm$ 0.016           & 0.169 $\pm$ 0.035         & 0.249 $\pm$ 0.064        & 0.7 $\pm$ 0.03                   & 0.75 $\pm$ 0.024        \\ 
                          & CC              & 0.147 $\pm$ 0.018           & 0.147 $\pm$ 0.021         & 0.199 $\pm$ 0.042        & 0.722 $\pm$ 0.034                & 0.756 $\pm$ 0.028       \\ 
                          & RAKEL           & 0.166 $\pm$ 0.035           & 0.124 $\pm$ 0.027         & 0.194 $\pm$ 0.069        & 0.724 $\pm$ 0.048                & 0.754 $\pm$ 0.04        \\ 
                          & RAKEL1           & 0.134 $\pm$ 0.013           & 0.101 $\pm$ 0.026        & 0.15 $\pm$ 0.063         & 0.737 $\pm$ 0.05                 & 0.783 $\pm$ 0.023       \\ 
PWSCCs + LPCs             & BR              & 0.148 $\pm$ 0.025           & 0.139 $\pm$ 0.033      & 0.254 $\pm$ 0.063        & 0.708 $\pm$ 0.046                & 0.762 $\pm$ 0.036       \\ 
                          & CC              & 0.168 $\pm$ 0.031           & 0.168 $\pm$ 0.045        & 0.272 $\pm$ 0.061        & 0.684 $\pm$ 0.054                & 0.723 $\pm$ 0.048       \\ 
                          & RAKEL           & 0.155 $\pm$ 0.023           & 0.103 $\pm$ 0.022        & 0.178 $\pm$ 0.031        & 0.729 $\pm$ 0.032                & 0.763 $\pm$ 0.030       \\ 
                          & RAKEL1           & 0.14 $\pm$ 0.027            & 0.094 $\pm$ 0.018         & 0.193 $\pm$ 0.063        & 0.727 $\pm$ 0.053                & 0.773 $\pm$ 0.042       \\ 
Multi-stage PWSCCs + LPCs & BR              & 0.153 $\pm$ 0.014           & 0.147 $\pm$ 0.022      & 0.266 $\pm$ 0.037        & 0.689 $\pm$ 0.035                & 0.75 $\pm$ 0.025        \\ 
                          & CC              & 0.142 $\pm$ 0.029           & 0.146 $\pm$ 0.023        & 0.254 $\pm$ 0.094        & 0.714 $\pm$ 0042                 & 0.764 $\pm$ 0.045       \\ 
                          & RAKEL           & 0.154 $\pm$ 0.022           & 0.11 $\pm$ 0.012         & 0.196 $\pm$ 0.062        & 0.739 $\pm$ 0.022                & 0.768 $\pm$ 0.025       \\ 
                          & RAKEL1           & \textbf{0.131 $\pm$ 0.012          } & \textbf{0.09 $\pm$ 0.014}                       & 0.173 $\pm$ 0.03         & \textbf{0.743 $\pm$ 0.026}               & \textbf{0.787 $\pm$ 0.018}      
\\ \hline
Baseline &               & 0.313           & 0.500      & 0.687        & NA                & NA
\\ \hline
MIML & &    0.182           & 0.132      & 0.223        & NA                & NA                        
                          
                          
                           \\ \hhline{========}
\end{tabular}
}
\end{table}



\subsection{Comparison with MIML}
In this chapter, ML learning is used to classify frog calls without syllable segmentation. Compared with the MIML learning (Table~\ref{tab:accuracy} in chapter \ref{cha:cha6MIML}), the ML classification has a better classification performance. Although extracting global features for the ML classification will lose some detailed information, most frog vocalisations can still be successfully described when using cepstral features.
Compared with other features, global cepstral features are often calculated from each windowed signal. Then, the statistical results, such as mean and standard deviation, of all the windowed signals are calculated, this process will compress the information in the time domain but keep the information of the frequency domain. Since most frogs tend to continuously make calls, the compression of the time domain information will not greatly affect the discriminability of the ceptral features. In contrast, MIML classification needs to conduct the syllable segmentation before feature extraction. However, the use of AED often cannot accurately segment frog calls with low energies, which greatly affects the performance.




\section{Summary and limitations}
Acoustic sensors are more widely used to monitor frog calling activity and species richness than the traditional field survey method. However, the use of acoustic sensors generates large volumes of audio data, which makes it necessary to develop automated methods. This paper proposes a novel method for detecting frog calling activity and species richness based on AED and ML learning. Specifically,
AED is the first step to calculate frog calling activity. Meanwhile, those 10-second recordings without frog calls are filtered out. For those recordings with frog calls, ML learning is further used for estimating frog species richness with multi-stage PWSCCs and LPCs. Finally, statistical analysis is utilised to reflect the relationship between frog calling activity/species richness with weather variables (mean temperature and rainfall). Experimental results show that our proposed method can accurately estimate frog calling activity/species richness and reflect their relationship with weather variables. Future work will focus on a wider frog call database, including a larger number of frog species, and frog calls collected over a longer period.


