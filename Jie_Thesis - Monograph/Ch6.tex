% Ch5.tex

\chapter{Multiple-instance multiple-label learning for the classification of frog calls with acoustic event detection}
\label{cha:cha6MIML}


\section{Overview}
\label{sec:intro}

This chapter presents a method for the classification of simultaneously vocalising frog species in low signal-to-noise ratio (SNR) recordings. In chapter \ref{cha:cha4EnhancedFeature} and \ref{cha:cha5WaveletFeature}, frog call classification is solved using a  signal-instance single-label (SISL) framework, which cannot reflect the nature of low SNR recordings. Most low SNR recordings often consist of multiple simultaneously animal vocal activities including frogs, birds, crickets, and so on. This character of low SNR recordings makes the multiple-instance multiple-label (MIML) learning a suitable classification framework. To be specific, individual frog syllables in one audio clip are regarded as \textit{multiple instance}, and the frog species included in that audio clip denote \textit{multiple labels}. 
The key part of this MIML classification framework for studying frog calls is to detect individual syllables in environmental recordings with multiple simultaneously vocalising frog species. After syllable detection, standard acoustic features and MIML classifiers can be used to perform the MIML classification.


To evaluate our proposed MIML classification framework, a representative sample of 342 10-seconds recordings was exported from the database and split into training and testing sets. The performance is evaluated based on the MIML learning measures. Experimental results demonstrate the MIML classification framework can be successfully adopt to classify multiple simultaneously vocalising frog species in low SNR recordings.



\section{Materials and methods}
Our frog call classification system consists of
four steps: signal processing, acoustic event detection, feature
extraction, and classification (Fig.~\ref{fig:flowchart}). Detailed description of each step is listed in the following parts. 

\begin{figure}[htb!]
\centering
\includegraphics[width=\textwidth]{image/Ch6/flowchart.pdf}
\caption{Flowchart of a frog call classification system using MIML learning}
\label{fig:flowchart}
\end{figure}



\subsection{Materials}
\label{chap5:Materials}
Digital recordings in this study were obtained with a battery-powered, weatherproof Song
Meter (SM2) box (Wildlife Acoustics, http://www.wildlifeacoustics.com/products/song-meter-sm2-birds). Recordings were two-channel, sampled at 22.05 kHz and saved in WAC4 format. In this study, a representative samples of 342 10-s recordings was selected to train and evaluate our proposed algorithm for predicting which frog species are present in a recording. All those examples were collected between 02/2014 to 03/2014, because it is the frog breeding season with high calling activity. All the species that are present in each 10-s recording were manually labelled by an ecologist who studies frog calls.
There are totally eight frog species in the recordings: Canetoad (CAD) 
($F_{0}$=560 Hz), Cyclorana novaehollandiae (CNE) ($F_{0}$=610 Hz), Limnodynastes terraereginae (LTE) ($F_{0}$=610 Hz), Litoria fallax (LFX) ($F_{0}$=4000 Hz), Litoria nasuta (LNA) ($F_{0}$=2800 Hz), Litoria rothii (LRI) ($F_{0}$=1800 Hz), Litoria rubella (LRA) ($F_{0}$=2300 Hz), and Uperolela mimula (UMA) ($F_{0}$=2400 Hz). Here, $F_{0}$ is the averaged dominant frequency for each frog species. Each recording contains between 1 and 5 species. Following the prior work \cite{briggs2012acoustic}, we assume that recordings without any frog calls can be filtered during acoustic event detection.


\subsection{Signal processing}
All the recordings were first re-sampled at 16 kHz and mixed to mono. A spectrogram was then generated by applying short-time Fourier transform (STFT) to each recording. Specifically, each recording was
divided into frames of 512 samples with 50\% frame overlap.
A fast Fourier transform was then performed on each frame with a Hamming window, which yielded amplitude values for 256 frequency bins, each spanning 31.25 Hz. The final
decibel values (S) were generated using 
\begin{equation}
S_{tf} = 20*log_{10}(A_{tf})
\end{equation}
where A is the amplitude value, $t=0,...,T-1$ and $f=0,...,F-1$ represent frequency and time index, $T$ and $F$ are 256 frequency bins and 625 frames, respectively. 

\subsection{Acoustic event detection for syllable segmentation}
Acoustic event detection (AED) aims to detect specified acoustic event in an audio data. In this study, we use AED for frog syllable segmentation. Since all the recordings are collected from the field, there is much overlapping vocal activity from different sources. Traditional methods for frog syllable segmentation are based on time domain information \cite{somervuo2004classification,huang2009frog}, which cannot address those recordings. Here, we modified the AED method developed by Towsey et al \cite{towsey2012toolbox} to segment syllables in those recordings with overlapping activity. The detail of our AED method is described as follows:
\\
\textbf{Step 1:} Wiener filtering 

\noindent To de-noise and smooth the spectrogram, a 2-D Wiener filter is applied to the spectrogram image over a $5 \times 5$ time-frequency grid, where the filter size is selected after consideration of trade-off between removing the background graininess and blurring the acoustic events.
\begin{equation}
\hat{S_{tf}} = \mu + \frac{(\sigma^{2}-\nu^{2})}{\sigma^{2}}(S_{tf}-\nu)
\end{equation}
where $\mu$ and $\sigma^{2}$ are local mean and variance, respectively; $\nu^{2}$ is the noise variance estimated by averaging all local variances. 
\\
\textbf{Step 2:} Spectral subtraction

\noindent After Wiener filtering, the graininess has been removed. However, some noises such as wind, insect, motor engine that cover the whole recording cannot be removed. Here, a modified spectral subtraction method is employed for dealing with those noises. 


\begin{algorithm}
\DontPrintSemicolon
\KwData{$\hat{S_{tf}}$, spectrogram after Wiener filtering.}
\KwResult{$\hat{S^{'}_{tf}}=\hat{S_{tf}}$, noise reduced spectrogram.}

\Begin{
%$V \longleftarrow U$\;
%$S \longleftarrow \emptyset$\;
\textbf{Construct} an array of the modal noise values for all frequency bins;

\For{$f\in F$}{
\textbf{1}. calculate the histogram of the intensity value over each frequency bin 

\textbf{2}. smooth the histogram array with a moving average window of size 7

\textbf{3}. regard the modal noise intensity at the position of maximal bin in the left-side of the histogram
}


\textbf{Smooth} the array with a moving average filter with window of size 5;

\For{$f\in F$}{
\textbf{1}. subtract the modal noise intensity

\textbf{2}. truncated negative decibel values to zero
}
}  
\caption{Modified Spectral Subtraction\label{IR}}

\end{algorithm}

\noindent \textbf{Step 3:} Adaptive thresholding

\noindent After noise reduction, the next step is to convert a noise reduced spectrogram $\hat{S^{'}_{tf}}$ into the binary spectrogram $S^{b}_{tf}$ for events detection. Here, an adaptive thresholding method named \textit{Otsu thresholding} \cite{otsu1975threshold} is employed to find an optimal threshold.

\begin{equation}
\phi_{b}^{2}(k)=w_{1}(k)w_{2}(k)[\mu_{1}(k)-\mu_{2}(k)]^{2}
\end{equation}
\noindent where $w_{1}(k)=\sum_{0}^{k}p(j)$ is calculated from the histogram as $k$, $p(j)=n(j)/N$ are the values of the normalized gray level histogram, $n(j)$ is the number of values in level $j$, $N$ is the total number of values over the whole spectrogram image, $\mu_{1}(k)=[\sum_{0}^{k}p(j)x(j)]/w_{1}$, $x(j)$ is the value at the center of the $j$th histogram bin. Then, the threshold, $T_{0}$, is calculated as

\begin{equation}
T_{0}= (\phi_{b1}^{2}(k) + \phi_{b2}^{2}(k)) / 2
\end{equation}

\noindent \textbf{Step 4:} Events filtering using dominant frequency and event area \\
Since not all detected events are correspond to frog vocalizations, to further remove those events that are from the listed frog species in section \ref{chap5:Materials}, dominant frequency ($F_{0}$) and area within the event boundary ($Ar$) are used for filtering.


\begin{algorithm}
\DontPrintSemicolon
\KwData{ $S^{b}_{tf}$, spectrogram; $t_{s}(n)$, $t_{e}(n)$, $f_{l}(n)$, $f_{h}(n)$, location of each acoustic event $n$; $F_{0}(i)$, dominant frequency of frog species $i$.}
\KwResult{ $\tilde{S_{tf}}$, spectrogram after events filtering.}

\Begin{
%\textbf{Step 1:} separate large acoustic events
%\\
\textbf{Calculate} the area of each acoustic event $n$.
$Area(n)=(t_{e}(n)-t_{s}(n))*(f_{h}(n)-f_{l}(n))$

\For{$n \in N_{e1}$}{
\If{$Ar(n) \geq Ar_{l}$}{
split event $n$ into small events
}
} 
where $Ar_{l}$ is set as 3000 pixels.\\
%\textbf{Step 2:} dominant frequency 
\textbf{Filter} events using dominant frequency
$f_{d}(n)= \sum_{t=t_{s}(n)}^{t_{e}(n)} F(t) / t_{e}(n)-t_{s}(n)$ \\
where $F(t)$ is the peak frequency of each frame within the event area


\For{$n \in N_{e2}$}{
\For{$i \in I$}{
\If{$f_{d}(n) \geq F_{0}(i)+\theta$; $f_{d}(n) \leq F_{0}(i)-\theta$}{
	$f_{d}(n) =0$;
}
}}
where $\theta$ is frequency range and set as 300 Hz.

\textbf{Remove} small acoustic events except frequency band between $\theta_{l}$ and $\theta_{h}$
\\
\For{$n \in N_{e2}$}{
\If{$Ar(n) \leq Ar_{s}$}{
remove event $n$
}
} 
where $Ar_{s}$ is set at 300 pixels, $\theta_{l}$ and $\theta_{h}$ are set as 300 Hz and 800 Hz, respectively. Because the area of LTE is smaller than $Ar_{s}$.
}
\caption{Event filtering based on dominant frequency and event area}
\end{algorithm}




\noindent \textbf{Step 5:} Region growing
\\
Region growing algorithm is utilised to obtain the contour of the particular acoustic event \cite{mallawaarachchi2008spectrogram}. To get the accuracy boundary of each acoustic event and improve the discrimination of extracted features, a 2-D region growing algorithm is applied to obtain the accuracy event shape within each segmented event. First, a maximal intensity value within the event area is selected as the seed point. Then, the neighbourhood pixels of the seed(s) above the threshold are located and assigned to the output image, and the new added pixels are used as seeds for further processing. Finally, when all the pixels that satisfy the criteria are added to the output image, the recursive algorithm will stop and get the final results (Fig.~\ref{fig:Ch6_AED}). Here, the threshold value is empirically set as 5 dB.

\begin{figure}[htb!]
\centering

        \begin{subfigure}[b]{0.45\textwidth}
                \includegraphics[width=\textwidth]{image/Ch6/AEoriginal.png}
                %\caption{ }
        \end{subfigure}
       ~
              \begin{subfigure}[b]{0.45\textwidth}
                \includegraphics[width=\textwidth]{image/Ch6/AEfinal.png}
                
        \end{subfigure}  
     
\caption[Acoustic event detection results before (Left) and after (Right) event filtering based on dominant frequency]{Acoustic event detection results before (Left) and after (Right) event filtering based on dominant frequency. Here, blue rectangle means the time and frequency boundary of each detected event.}
        \label{fig:feature}
\end{figure}

\begin{figure}[htb!]
\centering
        \begin{subfigure}[b]{0.45\textwidth}
                \includegraphics[width=\textwidth]{image/Ch6/binary.png}
                %\caption{ }
        \end{subfigure}
       ~
              \begin{subfigure}[b]{0.45\textwidth}
                \includegraphics[width=\textwidth]{image/Ch6/segmentEvents.png}                
        \end{subfigure}       
\caption[Acoustic event detection results after region growing]{Acoustic event detection results after region growing. Left: binary segmentation results; Right: segmented frog syllables.}
        \label{fig:Ch6_AED}
\end{figure}



\subsection{Feature extraction}
Based on acoustic event detection results, two feature representations are first calculated to describe each event (syllable): mask descriptors and profile statistic \cite{briggs2012acoustic}. Here, we exclude histogram of orientation from our feature set, because the previous studies have already demonstrated its lowest classification accuracy \cite{briggs2012acoustic, ruizmultiple2015}. For mask descriptors, it is used to describe the syllable shape including minimum frequency, maximum frequency, bandwidth, duration, area, perimeter, non-compactness, rectangularity. For profile statistics, there are time-Gini, frequency-Gini, frequency-mean, frequency-variance, frequency-skewness, frequency-kurtosis, frequency-max, time-max, mask-mean, and mask standard deviation. The third feature set consists of all features.







\subsection{Multiple-instance multiple-label classifiers}
After feature extraction, three MIML algorithms are evaluated for the classification of multiple simultaneous vocalising frog species: MIML-SVM, MIML-RBF, and MIML-kNN. With some form of event-level distance measure, the MIML problem has been reduced to a single-instance multiple-label problem by associating each event with a event-level feature \cite{briggs2012acoustic}. Here, the maximal and average Hausdorff distances between two syllables are used by MIML-SVM and MIML-RBF, separately. For MIML-kNN, the nearest neighbour is used to assign syllable-level features. 


\section{Experiment results}

\subsection{Parameter tuning}
There are three modules whose parameters need to be discussed: signal processing, acoustic event detection, and classification. For signal processing, the window size and overlap are 512 samples and 50\%, respectively. During the process of acoustic event detection, four thresholds for event filtering need to be determined, which are small and large area threshold, and frequency boundary for events filtering. All those thresholds were determined empirically by applying various combinations of thresholds to a small number of randomly selected 10s clips. For MIML-SVM classifiers, the parameters used are ($C,\gamma,r$) and set as (0.1, 0.6, 0.2) experimentally. For MIML-RBF, the parameters are ($ r, \mu$) and set as (0.1,0.6). For MIML-kNN, the number of references (k) and citers ($k^{'}$) are 10 and 20, respectively.
 
%Five measures including Hamming loss, rank loss, one-error, coverage, and micro-AUC are used to characterize the accuracy of each algorithm \cite{zhou2008miml, dimou2009empirical}. The definition of each measure can be found in \cite{briggs2012acoustic}
\subsection{Classification}
In this study, all the algorithms were programmed in Matlab 2014b. Each MIML algorithm is evaluated with five-fold cross-validation on the collection of 342 species-labelled recordings. 
Five evaluation rules are used for comparing the performance with the combination of three feature representations and three ML algorithms: Hamming loss, Rank loss, Average precision, One error, Exampled based F1, and Micro F1 \citep{Madjarov20123084, zhou2008miml}. The value range of all five evaluation rules is between 0 to 1. The definition of each evaluation rule is described as follows:
\\
\textbf{1)} Hamming loss directly measures the fraction of labels that are incorrectly predicted. A smaller value of hamming loss indicates a better classification performance. 
\\
\textbf{2)} Rank loss indicates the number of label pairs that are incorrectly ordered by the scores of ML classifier. A smaller value of rank loss indicates a better classification performance.
\\
\textbf{3)} Average precision is the average fraction of labels that are ranked higher than an actual label belonging to an example. A higher value of average precision indicates a better classification performance.
\\
\textbf{4)} One error is the fraction of bags whose top scoring label is not in the true label set. A smaller value of one error indicates a better classification performance.
\\
\textbf{5)}
Exampled based F1 is the average of the harmonic mean of example-precision and example-recall for every example. The example-precision is defined for an example as the size of the intersection of the set of its predicted labels and the set of its ground truth labels divided by the size of the set of its predicted labels. The example-recall is defined for an example as the size of the intersection of the set of its predicted labels and the set of its ground truth labels divided by the size of the set of its ground truth labels. A higher value of exampled based F1 indicates a better classification performance.
\\
\textbf{6)}
Micro F1 is the harmonic mean of micro-precision and micro-recall where micro-precision and micro-recall are the precision and the recall which are averaged over all example and label pairs. A higher value of micro F1 indicates a better classification performance.


The positive/negatives is $1-$Hamming loss and it is 0.818 for MIML-RBF with MD. Mask descriptors (MD) and profile statistical (PS), and all features (AF) are put into the three classifiers, respectively. The accuracy measure for each MIML classifier is shown in Table \ref{tab:accuracy}. Here, the best classification accuracy is achieved by MIML-RBF using MD. For each classifier, the classification accuracy of MD is higher than PS and AF, which shows that the event shape have higher discrimination power than the event content. To give a concrete view of predictions, the results of 5 randomly selected recordings using MIML-RBF are shown in Table \ref{tab:prediction}. Recordings of No.1 and No.3 are accurately predicted. 




\begin{table}[htb!]
\centering
\caption[Accuracy measure for MIML classifiers with different feature representations]{Accuracy measure for MIML classifiers with different feature representations. Here, $\downarrow$ indicates ‘the smaller the better’, while ‘$\uparrow$’ indicates ‘the bigger the better’.}
\label{tab:accuracy}
\resizebox{\textwidth}{!}{
\begin{tabular}{lllllll}
\hline\hline
{\bf Feature} & {\bf Algorithm} & {\bf Hamming loss $\downarrow$} & {\bf Rank loss $\downarrow$} & {\bf One-error $\downarrow$} & {\bf Coverage $\downarrow$} & {\bf Micro-AUC $\uparrow$} \\ \hline
MD            & MIML-SVM        & 0.253              & 0.186           & 0.308           & 3.147          & 0.745           \\ 
MD            & MIML-kNN        & 0.205              & 0.153           & 0.298           & 2.647          & 0.771           \\ 
MD            & MIML-RBF        & {\bf 0.182}        & {\bf 0.132}     & {\bf 0.223}     & {\bf 2.352}    & {\bf 0.828}     \\ 
PS            & MIML-SVM        & 0.239              & 0.208           & 0.323           & 3.544          & 0.728           \\ 
PS            & MIML-kNN        & 0.211              & 0.153           & 0.298           & 2.647          & 0.777           \\ 
PS            & MIML-RBF        & 0.186              & 0.161           & 0.338           & 3.161          & 0.746           \\ 
AF (MD+PS)            & MIML-SVM        & 0.261              & 0.199           & 0.279           & 3.588          & 0.761           \\ 
AF (MD+PS)               & MIML-kNN        & 0.205              & 0.160           & 0.264           & 2.735          & 0.787           \\ 
AF (MD+PS)              & MIML-RBF        & 0.191              & 0.142           & 0.220           & 2.632          & 0.821           \\ \hline\hline
\end{tabular}
}
\end{table}



\begin{table}[htb!]
\centering
\caption{Example predictions with MIML-RBF.}
\label{tab:prediction}
\begin{tabular}{lll}
\hline\hline
{\bf No.} &{\bf Ground truth} & {\bf Predicted labels} \\ \hline
1&UMA                & UMA                    \\ 
2&LNA, LRI, UMA      & LNA, LRA, UMA          \\ 
3&LNA, UMA           & LNA, UMA               \\ 
4&LNA, LFX, LRA      & LNA, LFX, LRI, LRA     \\ 
5&LNA, LFX, LRA      & LNA, LRA               \\ \hline\hline
\end{tabular}
\end{table}



\section{Discussion}
Since most recordings used in this study consist of multiple simultaneously vocalising frog species, the traditional SISL classification framework is no longer suitable. A novel framework for the classification of multiple simultaneous vocalising frog species in environmental recordings is proposed, which is adopt from \cite{briggs2012acoustic} that studied birds. Different from their work, we design a new acoustic event detection method for syllable segmentation rather than using a supervised learning algorithm. It is because we are lack of lots of annotated frog recordings. As for the classification results, our proposed framework can achieve a acceptable classification accuracy. However, the classification results are highly affected by the AED results. 


\section{Summary and future work}
In this study, we propose a novel framework for the classification of multiple simultaneous vocalising frog species in environmental recordings. To the best of our knowledge, this is the first study that applies the MIML algorithm to frog calls. Since frogs tend to call simultaneously, the MIML algorithm is more suitable for dealing with those recordings than single-instance single-label classification. After applying acoustic event detection algorithm to each 10s recording, each frog syllable is segmented. Then, three feature representations are calculated based on those segmented syllables. Finally, three MIML classifiers are used for the classification of frog calls with the best accuracy (81.8\% true positive/negatives). Future work will focus on the study of novel features and MIML classifiers for further improving the classification performance. To solve this problem, one solution is to prepare an annotated dataset and change to use supervised learning algorithm. Another is to used a different classification framework, which does not need the AED process.  


