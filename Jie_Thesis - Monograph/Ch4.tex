% Ch4.tex

\chapter[Frog call classification based on enhanced features]{Frog call classification based on enhanced features and machine learning algorithms}
\label{cha:cha4EnhancedFeature}



\section{Introduction}
\label{S:1}

This chapter presents an enhanced feature representation for the frog call classification using various machine learning algorithms. In the literature, various feature representations have been developed for frog call classification. However, most features used in the prior work are based on either temporal features, perceptual features, or cepstral features. Is it that a combination of three types of features can discriminate a wider variety of species that may share similar characteristics in either temporal, perceptual or cepstral information but not all?

This chapter aims to compare various feature representations with different machine learning techniques. Based on the classification performance, suggested features can then be transplanted to study low-SNR recordings. This chapter directly addresses sub-research question 1(a): Various acoustic features have been developed to classify frog calls in high SNR recordings; which of those features can be transplanted from high SNR recordings to low SNR recordings?

The performance of the proposed method is evaluated based on twenty-four frog species, which are geographically well distributed through Queensland, Australia. Five feature representations are compared via five machine learning algorithms. Classification results demonstrate a combination of temporal, spectral and cepstral features can achieve the best performance. Compared with temporal and spectral features, cepstral features can achieve a robust classification accuracy, but sensitive to the background noise. Therefore, in the next chapter, we aim to develop a robust cepstral features that are not sensitive to the background noise.



\section{Architecture of the classification system for frog calls}
Our frog call classification system consists of six steps (Figure.~ \ref{fig:Ch4_flowchart}): data description, syllable segmentation, pre-processing,  feature extraction, feature fusion, and classification. Detailed information of each step is shown in following subsections. Different from previous studies \citep{huang2009frog, Xie1504:Acoustic}, pre-processing is directly applied to the segmented syllables rather than continuous recordings.

\begin{figure}[htb!] % Example image
\center{\includegraphics[width=1\linewidth]{image/Ch4/flowchart.pdf}}
\caption{Flowchart of frog call classification system}
\label{fig:Ch4_flowchart} 
\end{figure}


\subsection{Data description}
In this study, twenty-four frog species, which are widespread in Queensland, Australia, are selected for experiments (Table~\ref{tab:Ch4_frogName}). All the recordings are obtained from David Stewart's CD with a sample rate of 44.10 kHz and saved in MP3 format \citep{CD}. Each recording includes one frog species with the duration ranging from eight to fifty-five seconds.

\begin{table}[htb!]
\centering
\caption{Summary of scientific name, common name, and corresponding code. Frog species name with asterisk means means that it needs to be smoothed before segmentation}
\resizebox{0.85\textwidth}{!}{
\begin{tabular}{cllc}
\hline\hline
\multicolumn{1}{l}{\textbf{No.}} & \textbf{Scientific-name}      & \textbf{Common-name}   & \multicolumn{1}{l}{\textbf{Code}} \\ \hline
1                                  & \textit{Assa darlingtoni}             & Pouched frog                                                     & ADI                                \\ 
2                                  & \textit{Crinia parinsignifera}        & Eastern Sign-bearing Froglet                                     & CPA                                \\ 
3                                  & \textit{Crinia signifera}             & Common eastern froglet                                           & CSA                                \\ 
4                                  & \textit{Limnodynastes convexiusculus} & Marbled frog                                                    & LCS                                \\ 
5                                  & \textit{Limnodynastes ornatus}        & Ornate burrowing frog                                            & LOS                                \\ 
6                                  & \textit{Limnodynastes tasmaniensis}*   & Spotted grass frog                                              & LTS                                \\ 
7                                  & \textit{Limnodynastes terraereginae}  & Northern banjo frog                                              & LTE                                \\ 
8                                  & \textit{Litoria caerulea}             & Australian green tree frog                                      & LCA                                \\ 
9                                  & \textit{Litoria chloris}              & Red-eyed tree frog                                              & LCS                                \\ 
10                                 & \textit{Litoria latopalmata}         & Broad-palmed frog                                                & LLA                                \\ 
11                                 & \textit{Litoria nasuta}               & Striped rocket frog                                             & LNA                                \\ 
12                                 & \textit{Litoria revelata}             & Revealed tree frog                                             & LEA                                \\ 
13                                 & \textit{Litoria rubella}              & Desert tree frog                                                 & LRA                                \\ 
14                                 & \textit{Litoria tyleri}               & Southern laughing tree frog                                     & LTI                                \\ 
15                                 & \textit{Litoria verreauxii verreauxii}           & Whistling tree frog                                           & LVI                                \\ 
16                                 & \textit{Mixophyes fasciolatus}        & Great barred frog                                               & MFS                                \\ 
17                                 & \textit{Mixophyes fleayi}             & Fleay's Barred Frog                                             & MFI                                \\ 
18                                 & \textit{Neobatrachus sudelli}*        & Painted burrowing frog                                           & NSI                                \\ 
19                                 & \textit{Philoria kundagungan}         & Mountain frog                                                     & PKN                                \\ 
20                                 & \textit{Philoria sphagnicolus}*        & Sphagnum frog                                                    & PSS                                \\ 
21                                 & \textit{Pseudophryne coriacea}        & Red-backed toadlet                                            & PCA                                \\ 
22                                 & \textit{Pseudophryne raveni}*          & Copper-backed brood frog                                        & PRI                                \\ 
23                                 & \textit{Uperoleia fusca}*             & Dusky toadlet                                                   & UFA                                \\ 
24                                 & \textit{Uperoleia laevigata}          & Smooth toadlet                                                 & ULA                                \\ \hline\hline
\end{tabular}
}
\label{tab:Ch4_frogName}
\end{table}



\subsection{Syllable segmentation based on an adaptive end point detection}
Each recording is made up of the continuous multiple calls of one frog species. For frogs, one syllable is an elementary acoustic unit for classification, which is a continuous frog vocalization emitted from an individual \citep{huang2009frog}. In this study, one method built on H$\ddot{a}$rm$\ddot{a}$'s method is used to perform syllable segmentation for frog calls \citep{harma2003automatic}. The syllable segmentation process is based on the spectrogram, which is generated by applying short-time Fourier transform (STFT) to each recording. For STFT, the window function used is Hamming window with the size and overlap being 512 samples and 25\%, respectively. The detail of the segmentation method is described in Figure.~\ref{fig:segmentation}, which is based on the iterative frequency-amplitude information of spectrogram. This paper focuses on the evaluation of fused features, but the accuracy of segmentation results can greatly affect the classification performance. To reduce the bias introduced by syllable segmentation, the segmented syllables are further filtered. First, those syllables whose length are smaller than 300 samples are removed. Then, those syllables whose averaged energy are smaller than 15\% of the maximum energy and larger than 1.5 times the averaged energy are removed for each frog species \citep{Gingras2013}.   


\begin{figure}[htb!] % Example image
\center{\includegraphics[width=1\linewidth]{image/Ch4/segmentation.pdf}}
\caption{Segmentation method based on H$\ddot{a}$rm$\ddot{a}$'s algorithm. Here, $D$ is the amplitude threshold for stopping criteria which is set at 20 dB experimentally, and the segmentation result is sensitive with this value. $A$ is the maximum amplitude value of the spectrogram and we save the first maximum amplitude as $A(1)$, $B(t)$ is the amplitude of frame $t$. An asterisk denotes the optional processing step.}
\label{fig:segmentation} 
\end{figure}



In this study, smoothing spectrogram is optionally applied to the spectrogram before H$\ddot{a}$rm$\ddot{a}$'s algorithm, because some frog species have large temporal gap within one syllable (see in Figure.~\ref{fig:smooth}). As for the smoothing, a Gaussian filter (7$\times$7) is applied to the spectrogram, where the size is set taking into account a trade-off between connecting gaps within one syllable and separating adjacent syllables. The segmentation result after smoothing is shown in Figure.~\ref{fig:smooth}. The distribution of syllable numbers after segmentation for all frog species is shown in Figure.~\ref{fig:Ch4_syllable}.

\begin{figure}[htb!]
\centering
        \begin{subfigure}[b]{0.6\linewidth}
                \includegraphics[width=\textwidth]{image/Ch4/no.pdf}
                \caption{Before smoothing}
        \end{subfigure}%
        \\ 
        \begin{subfigure}[b]{0.6\linewidth}
                \includegraphics[width=\textwidth]{image/Ch4/yes.pdf}
                \caption{After smoothing}
        \end{subfigure}
        \caption{Syllable segmentation results are marked with red
line for \textit{Neobatrachus sudelli} (one syllable).}       
        \label{fig:smooth}
\end{figure}



\begin{figure}[htb!] % Example image
\center{\includegraphics[width=0.6\linewidth]{image/Ch4/syllable.pdf}}
\caption{Distribution of syllable number for all frog species. The x-axis is the abbreviation of each frog species, and the corresponding scientific name can be found in Table \ref{tab:Ch4_frogName}.}
\label{fig:Ch4_syllable} 
\end{figure}


\subsection{Pre-processing}
Since features play an important role in the classification performance, pre-processing is applied to each syllable to improve the accuracy of feature extraction. The pre-processing of each syllable consists of the following steps:

\subsubsection{Pre-emphasise}
Some collected frog calls have low amplitude but in the high frequency, which will have an effect on feature extraction of the spectrum at the high end. To enhance those high-frequency components and reduce the low-frequency components, a first-order high-pass filter with finite pulse response (FIR) is introduced and defined as follows:

\begin{align}
y(n) = s(n)-\alpha s(n-1)
\end{align}

where $s(n)$ is a syllable of frog call, $y(n)$ is the output of the high-pass filter, $\alpha$ denotes the cut-off frequency of the high-pass filter and is set at 0.97 here, $n$ is the n-th sample of the syllable. 

\subsubsection{Windowing}
After pre-emphasise, each syllable is segmented into overlapping frames with fixed length. A Hamming widow is used to minimise the maximum side-lobe in the frequency domain and get side-lobe suppression, which is defined as

\begin{equation}
w(n)=0.54-0.46cos(\frac{2n\pi}{L-1}), 0 \leq n \leq L-1
\end{equation}
where $L$ is the length of the frame. Because window sizes have an effect on the classification results, different window sizes are optimised for different features in this study. The signal after window process is expressed as
\begin{equation}
x(n) = w(n)y(n)
\end{equation}

\subsection{Feature extraction}
After pre-processing of each syllable, various parametric representations are used to represent the syllable. In the literature, a variety of parametric representations of frog calls can be found, such as LPC and MFCCs \citep{yuan2012frog, jaafar2013automatic, bedoya2014automatic}.
Also, MFCCs achieved a better performance than LPC \citep{yuan2012frog}. Different from hybrid features used in \citep{huang2009frog, han2011acoustic, Gingras2013}, our enhanced enhanced feature consists of more features, such as oscillation rate \citep{Xie1504:Acoustic}, to further improve the classification accuracy. In this study, temporal features include syllable duration, Shannon entropy, r$\acute{e}$nyi entropy, zero-crossing rate, averaged energy, and oscillation rate. Perceptual features contains spectral centroid, spectral flatness, spectral roll-off, signal bandwidth, spectral flux, and fundamental frequency. MFCCs feature is used as a cepstral feature. The description of each feature is list below:

\vspace{3mm}

\noindent(1) Syllable duration ($Dr$): Syllable duration \citep{Xie1504:Acoustic} is directly obtained from the bounds (time domain) of the segmentation results.
\begin{equation}
Dr = x(n_{e}) - x(n_{s})
\end{equation}
where $n_{e}$ and $n_{s}$ are the end and start location of one segmented syllable.

\vspace{3mm}

\noindent(2) Shannon entropy ($Se$): Shannon entropy is the expected information content of a sequence of a signal. It is often used to describe the average of all the information contents weighted by their probabilities $p_{i}$.
\begin{equation}
Se=-\sum_{i=1}^{L}p_{i}log_{2}(p_{i})
\end{equation}
where $L$ is the legnth of a frog syllable.

\vspace{3mm}

\noindent(3) R$\acute{e}$nyi entropy ($Re$): r$\acute{e}$nyi entropy is calculated to obtain the different averaging of probabilities via the parameter $\alpha$, and defined as

\begin{equation}
Re=\frac{1}{1-\alpha}log_{2}(\sum_{i}^{n}p_{i}^{\alpha})
\end{equation}
where $p_{i}$ is the probabilities of the occurence x(n) in the signal.

\vspace{3mm}

\noindent(3) Zero-crossing rate ($Zcr$): zero-crossing rate denotes the rate of signal change along a signal. When adjacent signals have different signs, a zero-crossing occurs. The mathematical expression of $ZCR$ can be defined as
\begin{equation}
Zcr=\frac{1}{2}\sum_{n=0}^{L-1}[sgn(x(n))-sgn(x(n+1))]
\end{equation}

\vspace{3mm}

\noindent(4) Averaged energy (Ae): Averaged energy is defined as the sum of intensity of signal.
\begin{equation}
Ae = \frac{1}{L}\sum_{n=0}^{L-1}x(n)^{2}
\end{equation}


\vspace{3mm}
\noindent(5) Oscillation rate ($Or$): Oscillation rate is calculated in the frequency boundary around the fundamental frequency. First, the power within the frequency boundary is calculated. After normalising the power, the first and last 20\% part of the power vector is discarded due to the uncertain. Next, the autocorrelation is performed by the length of the vector. Furthermore, a discrete cosine transform is employed to the vector after mean subtraction, and the position of the highest frequency is achieved to calculate the oscillation rate. Detailed description can be found in our previous study \citep{Xie1504:Acoustic}.




\vspace{3mm}
\noindent(6) Spectral centroid ($Sc$): spectral centroid is the centre point of spectrum distribution. In terms of human audio perception, it is often associated with the brightness of the sound. With the magnitudes as the weight, it is calculated as the weighted mean of the frequencies.
\begin{equation}
Sc=\frac{\sum_{k=0}^{N-1}f_{k}X(k)}{\sum_{k=0}^{N-1}X(k)}
\end{equation}
where $X(k)$ is the discrete Fourier transform (DFT) of the syllable signal of the k-th frame, N is the half size of DFT. 

\vspace{3mm}
\noindent(7) Spectral flatness ($Sf$): spectral flatness provides a way to quantify the tonality of a sound. A higher spectral flatness indicates a similar amount of power of the spectrum in all spectral bands. Spectral flatness is measured by the ratio of the between the geometric mean and the arithmetic mean of the power spectrum and defined as
\begin{equation}
Sf = \frac{\sqrt{\frac{1}{N}\sum_{k=0}^{N-1}InX(k)}}{\frac{1}{N}\sum_{k=0}^{N-1}X(k)}
\end{equation}

\vspace{3mm}

\noindent(8) Spectral roll-off ($Sr$): spectral roll-off is often used to measure the spectral shape, and defined as the frequency $H$ below which $\theta$ of the magnitude distribution in concentrated.

\begin{equation}
\sum_{k}^{H}X(k)=\theta \sum_{k=1}^{N-1}X(k)
\end{equation}
where $\theta$ is set at 0.85.

\vspace{3mm}

\noindent(9) Signal bandwidth ($Bw$): signal bandwidth can be used to represent the difference between the upper and lower cut-off frequencies.

\begin{equation}
Bw=\sqrt{\frac{\sum_{k=0}^{N-1}(k-Sc)^{2}|x(n)|}{\sum_{k=0}^{N-1}X(k)}}
\end{equation} 

\vspace{3mm}

\noindent(10) Spectral flux ($Sf$): spectral flux is used to measure how quickly the power spectrum of a signal is changing. The spectral flux can be obtained via the power spectrum comparison between one frame and its previous one. The calculation of spectral flux is denotes as
\begin{equation}
Sf = \sum_{k=-\frac{N}{2}}^{k=\frac{N}{2}}H[|X(n,k)|-|X(n-1,k)|]
\end{equation}
where $H(x)=(x+|x|)/2$ is half-wave rectifier function.

\vspace{3mm}

\noindent(11) Fundamental frequency: fundamental frequency is calculated via averaging peak intensity of all frames within one frog syllable. If the peak intensity value is higher than an empirically chose or specified threshold, the frequency of that peak will be selected to calculate the fundamental frequency.

\vspace{3mm}

\noindent(12) Mel-frequency cepstral coefficients (MFCCs): MFCCs, which are obtained by applying cosine transform to a sub-band Mel-frequency spectrum within a short time, have been widely used in bird classification \citep{lee2006automatic}, speech/speaker recognition \citep{han2006efficient}, and frog identification \citep{bedoya2014automatic}. In this study, MFCCs are calculated based on the method of \citep{lee2006automatic}. 

\noindent  \textbf{Step 1}: Band-pass filtering: The amplitude spectrum is then filtered using a set of triangular band-pass filters.
\begin{equation}
E_{j}=\sum_{k=0}^{N/2-1}\phi_{j}(k)A_{k}, 0 \leq j \leq J-1
\end{equation}
where J is the number of filters, $\phi_{j}$ is the $j^{th}$ filter, and $A_{k}$ is the amplitude of $X(k)$.
\begin{equation}
A_{k}=|X[k]|^{2}, 0 \leq k \leq N/2
\end{equation}
\noindent  \textbf{Step 2}: Discrete cosine transform: MFCCs for the $i^{th}$ frame are computed by performing DCT on the logarithm of $E_{j}$. 
\begin{equation}
C_{m}^{j} = \sum_{j=0}^{J-1}\cos(m \frac{\pi}{J}(j+0.5))log_{10}(E_{j}), 0 \leq m \leq L-1
\end{equation}
where $L$ is the number of MFCCs.

In this study, the filter bank consists of 40 triangular filters, that is  $J=40$. The length of MFCCs of each frame is 12 (L=12). After calculating MFCCs from each frame, the averaged MFCCs of all frames within one syllable are calculated.
\begin{equation}
f_{m}= \frac{\sum_{i=1}^{K}(C_{m}^{l})}{K}, 0\leq m \leq L-1
\end{equation}
where $f_{m}$ is the $m^{th}$ MFCCs, $K$ is the number of frames within the syllable. 

For all perceptual features and $Zcr$, the mean values are calcualted to characterise the frog syllable. Then, the L-dimensional MFCC vectors are fused with other 11 feature vectors to form the enhanced temporal, perceptual and cepstral (TemPerCep) features.

After the formulation of feature vectors, the normalisation is conducted as follows
\begin{equation}
v_{i} = \frac{v_{i}-\mu_{i}}{\sigma_{i}}
\end{equation} 
where $\mu_{i}$ and $\sigma_{i}$ are the mean and standard deviation computed for each feature vector $i$.  

Let $F_{1}$ represent temporal features with length $L_{1}$, $F_{2}$ and $F_{3}$ represent perceptual features and cepstral features with length $L_{2}$ and $L_{3}$, respectively. The enhanced procedure is performed as
\begin{equation}
F_{H} = w_{1}F_{1}\oplus w_{2}F_{2} \oplus w_{3}F_{3}
\end{equation} 
where $w_{1}$, $w_{2}$, and $w_{3}$ are the weights, $\oplus$ is the concatenation operation.


\subsection{Classifier description}
In this paper we report the results for five classification algorithms: 1) Linear discriminant analysis (LDA), 2) K-nearest neighbour, 3) Support vector machines, 4) Random forest, 5) Artificial neural network. Five feature vectors, linear predictive coding, MFCCs, enhanced temporal feature and MFCC (\textit{TemCep}), enhanced temporal and perceptual features (\textit{TemPer}), enhanced temporal, perceptual features, and MFCC (\textit{TemCepPer}), are fed into each classifier respectively to test their classification performance.

\subsubsection{Linear discriminant analysis}
After transforming feature vector into low-dimensional space, the classification accuracy can be improved for linear discriminant analysis (LDA). In LDA, the goal is to find an optimal transformation matrix to transform the feature vector from an n-dimensional space to a d-dimensional space. A linear mapping, which maximises the Fisher criterion $J_{F}$, is used to obtain the transformation matrix as follow.
\begin{equation}
J_{F}(A)=tr((A^{T}S_{w}A)^{-1}(A^{T}S_{B}A))
\end{equation}
where $S_{w}$ and $S_{B}$ are the within-class scatter matrix and between-class scatter matrix, respectively. The within-class scatter matrix and between-class scatter matrix are respectively defined as 

\begin{equation}
S_{W}=\sum_{j=1}^{C}\sum_{i=1}^{N_{j}}(F_{i}^{j}-\mu_{j})(F_{i}^{j}-\mu_{j})^{T}
\end{equation}

\begin{equation}
S_{B}=\sum_{j=1}^{C}(\mu_{i}-\mu)(\mu_{i}-\mu)^{T}
\end{equation}
where $F_{i}^{j}$ is the i-th feature vector of frog species $j$, $\mu_{j}$ is the mean vector of species $j$, $C$ is the number of frog species, and $N_{j}$ is the number of feature vectors in species $j$, $\mu$ is the mean vector of all frog species.

The optimisation of the transform matrix can be determined via finding the eigenvectors of $S_{W}^{-1}S_{B}$.

\begin{equation}
A_{opt}= argmax \frac{tr(A^{T}S_{B}A)}{A^{T}S_{W}A}
\end{equation}

In the recognition stage, the feature vector is first transformed into a lower-dimensional space via $A_{opt}$ derived by LDA. Then, the distance between the feature vector of the test syllable and the feature vector representing this species is calculated. The one with minimum distance is regarded as the identified species.

\subsubsection{K-nearest neighbour}
For the K-NN classifier, the distance between an input frog feature vector and all stored feature vectors is first calculated. Then $K$ closest vectors are selected to determine the species of the input feature vector by majority voting. For example, the Euclidean distance between an input instance $i$ (frog feature vector) and one stored instance $j$ is calculated as 
\begin{equation}
d(i,j)=\sqrt{\sum_{c=1}^{n}(F_{i,c} - F_{j,c})^{2}}
\end{equation}

\noindent Then the species of this input instance $i$ can be predicted from the selected $k$ nearest neighbours. If 

\begin{equation}
\frac{1}{k_{1}}\sum_{j\epsilon S_{1}}d(i,j(S_{1})) \leq \frac{2}{k_{2}}\sum_{j\epsilon S_{2}}d(i,j(S_{2}))
\end{equation}
where $k = k_{1}+k_{2}$, $k_{1}$ is the number of frog species $S_{1}$, $k_{2}$ is the number of frog species $S_{2}$. Here the input instance $i$ will be classified as frog species $S_{2}$. Following prior work (\citep{han2011acoustic, Xie1504:Acoustic}), the distance function used for K-NN is the Euclidean function, and $k$ is set at 1.

\subsubsection{Support vector machines}
Due to the high accuracy and superior generalization properties, support vector machines (SVM) have been widely used for classifying animal sounds \citep{huang2009frog} \citep{acevedo2009automated}. In this study, the feature set obtained is first selected as training data. Then, the pairs $(F_{l}^{n},L_{l}^{n}), l=1,2,..., C_{l}$ are constructed using the selected training data, where $C_{l}$ is the number of frog instance in the training data, $F_{l}^{n}$ is the feature vector obtained from the $l$-$th$ frog instance in the training data, $L_{l}^{n}$ is the frog species label. Furthermore, the decision function for the classification problem based on SVM \citep{cortes1995support} is defined by the training data as follows.
\begin{equation}
f(v) = sgn(\sum_{sv}\alpha_{l}^{n}L_{l}^{n}K(v,v_{l}^{n})+b_{l}^{n})
\end{equation}

where $K(.,.)$ is the kernel function, $\alpha_{l}^{n}$ is the Lagrange multiplier, and $b_{l}^{n}$ is the constant value. In this work, the Gaussian kernel is selected as the kernel function. Parameters $\alpha$ and $v$ are selected independently for each feature vector by grid search using cross-validation \citep{hsu2003practical}.


\subsubsection{Random forest}
Random forest (RF) is a tree-based algorithm, which builds a specified number of classification trees without pruning. The nodes are split on a random drawing of $m$ features
from the entire feature set $M$. A bootstrapped random sample from the training set is used to build each tree. The advantage of RF is its ability to generate a metric to rank predictors based on their relative contribution to the model's predictive accuracy \citep{bao2005prediction}. The prediction is defined as follows.
\begin{equation}
Pred = \frac{1}{K}\sum_{n=1}^{K}T_{i}
\end{equation}
where $T_{i}$ is the n-th tree response of the RF. In this work, the number of trees $K$ is set at 300 trees to characterise frog calls. As for the predictor variables $m$, it is set at $\sqrt{N}$, where $N$ is the feature dimension in a syllable. 

\subsubsection{Artificial neural network}
Artificial neural network (ANN) is a non-linear, adaptive, machine learning tool with great capabilities for learning, generalization, non-linear approximation, and classification. An ANN architecture often consists of many interconnected neurons organised in successive layers: pattern layer, summation layer, and decision layer. The neuron in class is often computed by a Gaussian function. Then, the summation layer used summation units to memorise the class conditional pdfs of each class through a combination of Gaussian densities. Lastly, the decision layer unit classifies the pattern in accordance with the Bayesian decision rule based on the output of all summation layer neurons as follows.
\begin{equation}
D(F)=argmax{p_{i}(F)}, i =1,...,N
\end{equation}
where $i$ is the species index, $N$ is the total number of frog species.
\begin{equation}
p_{i}(F)=\sum_{j=1}^{m_{i}}\beta_{ij}\phi_{ij}(F)
\end{equation}
where $m_{i}$ is the number of Gaussian components, $\beta_{ij}$ and $\phi_{ij}(F)$ can be represented as follows.

\begin{equation}
\sum_{j=1}^{m_{i}}\beta_{ij}=1
\end{equation}

\begin{equation}
\phi_{ij}(F)=\frac{1}{(2\pi)^(d/2)\sigma^{d}}exp[-\frac{(F-\mu_{ij})^{T}(F-\mu_{ij})}{2\sigma^2}]
\end{equation}
where $i=1,...,N$, $j=1,...,m_{i}$, $d$ denotes the dimension of the input vector $F$, $\sigma$ is the smoothing parameter, $\mu_{ij}$ is the mean vector and the central of the classification. In this study, one ANN classifier named multiple perception layer (MLP) is used to classify frog calls.

\section{Experiment results}
In this experiment, all the dataset have been divided as 50\% of training set and 50\% testing set. For the testing phase, each machine learning algorithm is used to learn a model to classify frog calls with 5-fold cross validation. The performance of the proposed frog call classification system is evaluated by quantitatively expressed detection metrics, such as average accuracy, precision, and specificity. The definition of accuracy, precision, and specificity can be defined as

\begin{equation}
Sensitivity = \frac{TP}{TP+FN}
\end{equation}

\begin{equation}
Specificity = \frac{TN}{TN+FP}
\end{equation}

\begin{equation}
Accuracy = \frac{TP+TN}{TP+TN+FP+FN}
\end{equation}
where $TP$ is true positive, $FP$ is true positive, $TN$ is true negative, and $FN$ is false negative.

\subsection{Effects of different machine learning techniques}
Figure.~\ref{diffClassifier} shows the frog call classification performance with different machine learning techniques. The high classification results in term of the accuracy, sensitivity and specificity measure of different classifiers indicates good classification performance. It can be observed than RF achieves the best classification performance, while the classification performance of LDA is the lowest. Meanwhile, the classification performances of SVM and MLP are very good, which might be that the features and classifiers are quite suitable. It can be seen from Figure.~\ref{diffClassifier} that frog call classification with different machine learning techniques can achieve good performance with our enhanced feature representation, because the classification accuracy are very high. It can also be noted that RF can be highly recommended for classification of frog calls due to the highest classification accuracy.



\begin{figure}[htb!] % Example image
\center{\includegraphics[width=0.6\linewidth]{image/Ch4/diffClassifier.pdf}}
\caption{Classification results of different classifiers}
\label{diffClassifier} 
\end{figure}

\subsubsection{Effects of different feature sets}
Figure.~\ref{diffFeature} illustrates the classification accuracy with different feature sets: LPC feature, MFCCs (\textit{Cep}), temporal features and MFCCs (\textit{TempCep}), temporal features and perceptual features (\textit{TemPer}), and temporal features, perceptual features and MFCCs (\textit{TemPerCep}). It can be seen that cepstral features (\textit{Cep}, \textit{TempCep}, \textit{TemPerCep}) have more stable performance than LPC and perceptual features. It is evident that our proposed enhanced feature (\textit{TemPerCep}) show outstanding performance of all proposed feature representations of all the machine learning techniques. The reason for the high classification accuracy is that frog calls are short duration and cover a small spectral band. Our proposed enhanced feature, \textit{TemPerCep}, can better characterise the content of frog calls. Although the classification performance of \textit{TemPerCep} is not significantly higher than other feature representations, the difference does show that our proposed feature set is suitable and effective for the classification of frog calls.   


\begin{figure}[htb!] % Example image
\center{\includegraphics[width=0.6\linewidth]{image/Ch4/diffFeature.pdf}}
\caption{Classification results with different feature sets}
\label{diffFeature} 
\end{figure}

\subsubsection{Effects of different window size for MFCCs and perceptual features}
As we know, the window size have an effect on the MFCCs and perceptual features. Therefore, different window size will lead to different classification performance (Figure.~\ref{diffWinSize1} and Figure.~\ref{diffWinSize2}). The window size used for test is 32, 64, 128, 256, because the syllable length of some frog species is less than 512 samples.
It is found that the best classification performance for MFCCs is achieved with window size of 64 samples. For \textit{TemPer}, the window size of 64 obtains the best classification performance. 
It also can be observed that SVM and RF achieve the best classification performance. Moreover, different window sizes of MFCCs have a larger variation than \textit{TemPer} features, which  might be that temporal features have a high weight in \textit{TemPer} for the classification task.

\begin{figure}[htb!] % Example image
\center{\includegraphics[width=0.6\linewidth]{image/Ch4/windowSizeMFCC.pdf}}
\caption{Classification results of MFCCs with different window size}
\label{diffWinSize1} 
\end{figure}

\begin{figure}[htb!] % Example image
\center{\includegraphics[width=0.6\linewidth]{image/Ch4/windowSizeTemPer.pdf}}
\caption{Classification results of TemPer with different window size}
\label{diffWinSize2} 
\end{figure}

\subsubsection{Effects of noise}
To further evaluate the robustness of our proposed feature set, white noise with different signal-to-noise (SNR) of 40 dB, 30 dB, 20 dB, 10 dB, 0dB, and -10 dB is added to the frog calls. Because this paper focuses on the evaluation of features rather than the segmentation method, the artificial noise is added after syllable segmentation. Since SVM has shown a good performance for frog call classification in 3.3.1, we only use SVM to test the effects of different levels of artificial noise. The classification results of different levels of noise contamination are shown in Figure.~\ref{diffNoise}. It is found from Figure.~\ref{diffNoise} that MFCCs (Cep) are very sensitive to the background noise, compared with other feature representations. Comparing \textit{TemCep} with \textit{TemPer}, it can be observed that perceptual features have a better anti-noise ability than cepstral feature. It is also found that LPC has a good anti-noise ability when SNR is larger than 10, but the classification accuracy quickly decreases when SNR is smaller than 10. 


\begin{figure}[htb!] % Example image
\center{\includegraphics[width=0.6\linewidth]{image/Ch4/affectNoise.pdf}}
\caption{Sensitivity of different features for different levels of noise contamination}
\label{diffNoise} 
\end{figure}


\section{Discussion}
Table~\ref{my-label} shows the classification performance of previous methods. Since previous studies often used different dataset to perform the classification task, we implement all those features and applied them to our used dataset with the same classifier (SVM). Compared with those previous methods, our proposed enhanced feature representation significantly outperforms other methods. Therefore, we can conclude that our results stands above the current classification performance. From the table, we can also observe that MFCCs is the most popular feature that have been used for frog call classification. Among all used machine learning techniques, SVM shows the superior performance and are widely used for the classification task. It can be found that the classification accuracy of \textit{TemPerCep} does not show significat improvement when compared with MFCCs. However, combining temporal and perceptrual features with cepstral features greatly improves the antinoise ability of MFCCs.

\begin{table}[h]
\centering
\caption{Comparision with previous used feature representations}
\label{my-label}
\resizebox{\textwidth}{!}{
\begin{tabular}{lll}
\hline\hline
Ref. & Feature                                                                                                                                                           & Accuracy \\ \hline
 \citep{juan2009frogs, yuan2012frog}    & LPCs                                                                                                                                                              &     93.5\%     \\ 
\citep{lee2006automatic, Xie1504:Acoustic, jaafar2013automatic, bedoya2014automatic}  & MFCCs                                                                                                                                                             &     94.9\%     \\ 
\citep{han2011acoustic}     & \begin{tabular}[c]{@{}l@{}} Spectral centroid, Shannon entropy, \\ R$\acute{e}$nyi entropy \end{tabular}  &    75.6\%      \\ 
\citep{Xie1504:Acoustic}    & \begin{tabular}[c]{@{}l@{}}Syllable duration, dominant frequency, \\ oscillation rate, frequency modulation, \\ energy modulation     \end{tabular}                        &    92.3\%      \\ 
 \citep{Huang20141}    & \begin{tabular}[c]{@{}l@{}}Spectral centroid, signal bandwidth, \\  spectral roll-off,  threshold-crossing rate, \\ spectral flatness, and average energy\end{tabular} &  95.8\%        \\  
 Our feature representation    &                                                                                                                                        \textit{TemPerCep} &    99.1\%      \\ \hline\hline
\end{tabular}
}
\end{table}



\section{Conclusion and future work}
In this paper we proposed a novel enhanced feature representation to classify frog calls with various machine learning techniques. After segmenting continuous recordings into individual syllables, a variety of acoustic features are extracted from each syllable. Then, different features are fused to form different feature representations. Finally, various machine learning techniques are used to classify frog calls with different feature representations.
Our proposed enhanced feature representation shows the best classification accuracy and has good auti-noise ability. Meanwhile, the SVM and RF outperforms the traditional LDA and K-NN classifiers. Therefore, it is suitable to combine \textit{TemPerCep} with SVM or RF to build a frog call classification system. Ecologists can apply the proposed classification system to long-term frog recordings. Then, the long-term change of frog species richness can be reflected by the classification results. 

In the future, since MFCCs feature shows a good classification performance, but a bad anti-noise ability, we can modify MFCCs to improve the anti-noise ability. After transforming  frog audio data into its spectrogram representation, the visual inspection motivates us to use image processing techniques for studying frog calls. Also, a wider variety of frog audio data from different geographical and environmental conditions will be test in the future experiments.
